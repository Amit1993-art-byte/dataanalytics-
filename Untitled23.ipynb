{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM0xx6evJxMHqyUxXqAei5g"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"GRNMXF85d8Y6"},"outputs":[],"source":["# ASSIGNMENT                                   STATISTICS ADVANCE - 1"]},{"cell_type":"code","source":["# Q 1. Explain the properties of the F-distribution.\n","\n","# ANS:\n","\n","# The F-distribution, also known as Snedecor's F-distribution or the Fisher-Snedecor distribution,\n","# is a continuous probability distribution that arises in the context of ANOVA (Analysis of Variance)\n","# and other statistical tests. Here are its key properties:\n","\n","# 1. Non-negative Values:  The F-distribution is defined only for non-negative values.\n","#    It's impossible to have a negative F-statistic.\n","\n","# 2. Skewed to the Right: The F-distribution is always skewed to the right (positively skewed).\n","#    This means its tail extends further to the right than to the left.  The degree of skewness\n","#    decreases as the degrees of freedom increase.\n","\n","# 3. Two Degrees of Freedom:  Unlike some other distributions, the F-distribution has two degrees\n","#    of freedom parameters, usually denoted as ν₁ (numerator degrees of freedom) and ν₂\n","#    (denominator degrees of freedom). These parameters dictate the shape of the distribution.\n","\n","# 4. Dependence on Degrees of Freedom:  The shape of the F-distribution is heavily influenced by\n","#    the values of ν₁ and ν₂. As the degrees of freedom increase, the distribution becomes more\n","#    symmetrical and approaches a normal distribution.\n","\n","# 5. Used in Hypothesis Testing:  The F-distribution is crucial for F-tests, which are used to\n","#    compare variances or determine if there are statistically significant differences among means\n","#    of multiple groups.\n","\n","\n","# Example:\n","# Imagine you're comparing the variance of test scores between two different teaching methods.\n","# You'd calculate an F-statistic, and the F-distribution would help you determine the probability\n","# of observing such a large difference in variances if the teaching methods were actually equivalent.\n","# The critical F-value, derived from the F-distribution with appropriate degrees of freedom,\n","# would be used to make a decision about whether to reject the null hypothesis (that the variances\n","# are equal).\n","\n","\n"],"metadata":{"id":"8I4iUBZ6eAz5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Q 2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?\n","\n","# ANS:\n","\n","# The F-distribution is primarily used in statistical tests where the ratio of variances is of interest.  This makes it particularly suitable for:\n","# 1. Analysis of Variance (ANOVA): ANOVA tests compare the means of three or more groups.\n","#   The F-statistic in ANOVA is the ratio of the variance between the groups to the variance within the groups.\n","#   A large F-statistic suggests that the variation between group means is substantial compared to the variation within each group,\n","#   providing evidence against the null hypothesis (that all group means are equal).  The F-distribution helps determine the probability of observing such a large\n","#   F-statistic if the null hypothesis were true.\n","\n","# 2. Comparing Two Variances:The F-test can directly compare the variances of two populations. The F-statistic is the ratio of the two sample variances.\n","#    The F-distribution helps determine the likelihood of observing such a large ratio if the population variances were, in fact, equal.\n","\n","# Why is the F-distribution appropriate?\n","\n","# The F-distribution is appropriate for these tests because:\n","\n","# Ratio of Variances: It's specifically designed to model the distribution of ratios of variances. This is a direct consequence\n","# of its definition as the ratio of two chi-square variables divided by their respective degrees of freedom.\n","\n","# Degrees of Freedom: The two degrees of freedom parameters of the F-distribution allow it to accommodate the different sample sizes and\n","# numbers of groups involved in the tests.  Different sample sizes affect the variability of the sample variances, and these effects are\n","# captured by the degrees of freedom.\n","\n","# Hypothesis Testing Framework:  The F-distribution provides a framework for hypothesis testing. By comparing the calculated F-statistic\n","# to the critical value from the F-distribution (at a chosen significance level), one can determine the statistical significance of\n","# the observed differences in variances or means.\n"],"metadata":{"id":"UkAoNOSUfe31"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Q 3. What are the key assumptions required for conducting an F-test to compare the variances of two\n","#      populations?\n","\n","# 1. Normality:  The populations from which the samples are drawn must be approximately normally distributed.  While the F-test is\n","#   relatively robust to deviations from normality, especially with larger sample sizes, significant departures from normality can affect\n","#   the validity of the results.\n","\n","# 2. Independence: The two samples must be independent of each other. The observations in one sample should not be related to or influence\n","#   the observations in the other sample.\n","\n","# 3. Homoscedasticity (Equal Variances -  under the null hypothesis): Crucially, the F-test assumes that the population variances are equal.\n","#   This is the variance being tested, so the null hypothesis is that the variances are equal. The F-test is used to determine whether\n","#   there is enough evidence to reject this assumption.  It's important to remember that the F-test is designed to test this\n","#   specific assumption. If you violate this assumption, then your results will not be valid.\n"],"metadata":{"id":"sA_xgvwf5Qxi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Q 4. What is the purpose of ANOVA, and how does it differ from a t-test?\n","\n","# ANS:\n","\n","# ANOVA (Analysis of Variance) is a statistical test used to compare the means of three or more groups.  It determines if there is\n","# a statistically significant difference among the group means.  A t-test, on the other hand, is used to compare the means of two groups.\n","\n","# Here's a breakdown of their differences:\n","\n","# Number of Groups: ANOVA handles three or more groups; a t-test is for two groups.\n","\n","# Purpose: ANOVA tests for overall differences among multiple group means.  A t-test assesses the difference between two specific group means.\n","\n","# F-statistic vs. t-statistic: ANOVA uses the F-statistic, which is the ratio of variance *between* groups to the variance within groups.\n","# A t-test uses the t-statistic, which reflects the difference between the two group means relative to the variability within the groups.\n","\n","# Post-hoc Tests: If ANOVA shows a significant difference among group means, post-hoc tests (e.g., Tukey's HSD, Bonferroni) are often\n","# used to determine which specific pairs of group means differ significantly.  A t-test doesn't require post-hoc tests because it only\n","# compares two groups directly.\n","\n","\n","# In summary,  use ANOVA when comparing three or more groups; use a t-test when comparing only two.\n"],"metadata":{"id":"bgnHu0JQ5RvW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Q 5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more\n","#      than two groups.\n","\n","# ANS:\n","\n","# You would use a one-way ANOVA instead of multiple t-tests when comparing the means of three or more groups\n","# because multiple t-tests inflate the family-wise error rate.\n","\n","# Family-wise error rate (FWER): The probability of making at least one Type I error (false positive)\n","# across all the comparisons being made.\n","\n","# When conducting multiple t-tests, the probability of finding a statistically significant difference\n","# between at least one pair of groups purely by chance increases dramatically. Each individual t-test\n","# has a significance level (alpha, usually 0.05), meaning there's a 5% chance of incorrectly rejecting\n","# the null hypothesis (that there's no difference) in that specific comparison.  With multiple t-tests,\n","# these individual error rates accumulate.\n","\n","# Example: if you perform 6 t-tests (comparing 4 groups), the chances of making at least one Type I error\n","# is substantially higher than 0.05.\n","\n","# ANOVA, in contrast, assesses all groups simultaneously. It tests the null hypothesis that all the group\n","# means are equal. If the ANOVA finds a significant difference, it suggests that at least one group\n","# mean is different from the others, but it doesn't specify which pairs differ.  To determine which\n","# specific group means differ, post-hoc tests like Tukey's Honestly Significant Difference (HSD) or\n","# Bonferroni correction are used. These tests adjust the significance level to control the FWER, preventing\n","# an inflated chance of falsely concluding that there are differences between groups.\n","\n","# In summary:\n","\n","# 1. Use one-way ANOVA when comparing three or more groups to control the FWER. Multiple t-tests lead to\n","#    an inflated chance of Type I errors.\n","# 2. Use post-hoc tests after ANOVA to determine which specific group means are significantly different."],"metadata":{"id":"VAvadcVu5S9V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.\n","#    How does this partitioning contribute to the calculation of the F-statistic?\n","\n","# ANS:\n","\n","# In ANOVA, the total variance observed in the data is partitioned into two components:\n","\n","# 1. Between-Group Variance: This represents the variability among the means of different groups.  It measures how much the group\n","#     means deviate from the overall mean of all observations.  A large between-group variance suggests that the groups are distinct from\n","#     each other.\n","\n","# 2. Within-Group Variance: This represents the variability of individual observations within each group.  It measures the spread\n","#     or dispersion of data points around the mean of their respective groups. A small within-group variance suggests that the data\n","#     points within each group are tightly clustered around their group mean.\n","\n","\n","# The partitioning of variance can be visualized like this:\n","\n","# Total Variance = Between-Group Variance + Within-Group Variance\n","\n","# Calculating the F-statistic:\n","\n","# The F-statistic is the ratio of the between-group variance to the within-group variance:\n","\n","# F = Between-Group Variance / Within-Group Variance\n","\n","\n","# Interpretation:\n","\n","#  A large F-statistic indicates that the between-group variance is substantially larger than the within-group variance. This\n","#   suggests that the differences among the group means are more prominent than the random variation within each group, providing evidence\n","#   against the null hypothesis (that all group means are equal).\n","\n","#  A small F-statistic suggests that the between-group variance is not much larger than the within-group variance. This implies that the\n","#   differences among the group means might be due to random chance, and there is not enough evidence to reject the null hypothesis.\n","\n","\n","# The F-distribution then helps to determine the probability of observing such an F-statistic (or one even more extreme) if the null\n","#  hypothesis were true.  If this probability is below the significance level (alpha), we reject the null hypothesis."],"metadata":{"id":"Dk0n9AYnffB-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Q 7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key\n","#      differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?\n","\n","# Classical (Frequentist) ANOVA vs. Bayesian ANOVA\n","\n","# Classical ANOVA\n","#  Uncertainty:  Uncertainty is represented by probability distributions of test statistics under the null hypothesis.  It does not assign\n","#   probabilities to model parameters (e.g., group means).\n","\n","#  Parameter Estimation:  Parameter estimates (e.g., means, variances) are point estimates (sample means, sample variances)  considered\n","#   to be the \"best\" guess of the true value, with confidence intervals giving a range of plausible values.\n","\n","#  Hypothesis Testing: Based on p-values, tests the null hypothesis (e.g., no difference in group means).  A small p-value leads to\n","#   rejecting the null hypothesis in favor of the alternative (that there is a difference). The p-value is interpreted as the probability of\n","#   observing the data (or more extreme data), given that the null hypothesis is true.  Does not give the probability of the null hypothesis\n","#   being true.\n","\n","\n","# Bayesian ANOVA\n","# * Uncertainty: Uncertainty in model parameters (e.g., group means) is fully represented through probability distributions (posterior distributions).  These distributions express the degree of belief about the parameter values given the data. Prior distributions encode prior beliefs.\n","# * Parameter Estimation: Parameters are estimated via their posterior distributions.  The posterior distribution represents the updated belief about the parameters *after* considering the data. The posterior mean, median, or mode might be used as point estimates, but the entire distribution is relevant.  Credible intervals (e.g. 95% credible interval) represent a range of plausible parameter values. The credible interval reflects the probability that the true parameter lies within the interval.\n","# * Hypothesis Testing:  Instead of p-values and hypothesis rejection, Bayesian inference typically deals with Bayes factors (which compare different models/hypotheses) or posterior probabilities of hypotheses.   Posterior probabilities provide direct measures of belief in different hypotheses given the data.  This provides a more direct way to address model comparison.\n","\n","\n","# Key Differences Summarized:\n","\n","# | Feature        | Frequentist ANOVA                         | Bayesian ANOVA                                    |\n","# |----------------|--------------------------------------------|---------------------------------------------------|\n","# | Uncertainty    | Probability distribution of test statistic  | Probability distributions of parameters            |\n","# | Parameter Est. | Point estimates with confidence intervals   | Posterior distributions, credible intervals        |\n","# | Hypothesis Test| p-values, rejection of null hypothesis    | Bayes factors, posterior probabilities of hypotheses |\n","# | Interpretation | Probability of data given null hypothesis | Probability of hypothesis given the data           |"],"metadata":{"id":"fYozD84zffF7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#  Q 8. You have two sets of data representing the incomes of two different professions:\n","#       Profession A: [48, 52, 55, 60,62]\n","#       Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions'\n","#       incomes are equal. What are your conclusions based on the F-test?\n","#       Task: Use Python to calculate the F-statistic and p-value for the given data.\n","#       Objective:  Gain experience in performing F-tests and interpreting the results in terms of variance comparison\n","\n","# import scipy.stats as stats\n","\n","# Income data for Profession A and Profession B\n","# profession_a = [48, 52, 55, 60, 62]\n","# profession_b = [45, 50, 55, 52, 47]\n","\n","# Perform F-test\n","# f_statistic, p_value = stats.f_oneway(profession_a, profession_b)\n","\n","# print(\"F-statistic:\", f_statistic)\n","# print(\"P-value:\", p_value)\n","\n","# Interpretation (example, adjust alpha based on your needs):\n","# alpha = 0.05\n","# if p_value > alpha:\n","#    print(\"Fail to reject the null hypothesis. There is not enough evidence to suggest that the variances of the two professions' incomes are significantly different.\")\n","# else:\n","#   print(\"Reject the null hypothesis. There is significant evidence to suggest that the variances of the two professions' incomes are different.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rpy8aKn7eA3Q","executionInfo":{"status":"ok","timestamp":1729656212232,"user_tz":-330,"elapsed":17,"user":{"displayName":"AMIT KUMAR","userId":"09135217214384575547"}},"outputId":"28de5bb8-fa83-475a-8d05-bed44d65cd12"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["F-statistic: 3.232989690721649\n","P-value: 0.10987970118946545\n","Fail to reject the null hypothesis. There is not enough evidence to suggest that the variances of the two professions' incomes are significantly different.\n"]}]},{"cell_type":"code","source":["#  Q.9  Conduct a one-way ANOVA to test whether there are any statistically significant differences in\n","#       average heights between three different regions with the following data :\n","#       Region A:[160,162,165,158,164]\n","#       Region B:[172,175,170,168,174]\n","#       Region C:[180,182,179,185,183]\n","#       Task : Write Python code to perform the one-way ANOVA and interpret the results.\n","#       Objective : Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value.\n","\n","# import statsmodels.formula.api as sm\n","# from statsmodels.stats.anova import anova_lm\n","# import pandas as pd\n","\n","# Create a DataFrame\n","# data = {'region': ['A'] * 5 + ['B'] * 5 + ['C'] * 5,\n","#        'height': [160, 162, 165, 158, 164, 172, 175, 170, 168, 174, 180, 182, 179, 185, 183]}\n","# df = pd.DataFrame(data)\n","\n","# Perform one-way ANOVA\n","# model = sm.ols('height ~ C(region)', data=df).fit()\n","# anova_table = anova_lm(model, typ=2)\n","# print(anova_table)\n","\n","# Interpret the results\n","# alpha = 0.05  # significance level\n","# if anova_table['PR(>F)'][0] < alpha:\n","#    print(\"There is a statistically significant difference in average heights between the regions.\")\n","# else:\n","#    print(\"There is no statistically significant difference in average heights between the regions.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZIZz59UpeA76","executionInfo":{"status":"ok","timestamp":1729655759535,"user_tz":-330,"elapsed":1453,"user":{"displayName":"AMIT KUMAR","userId":"09135217214384575547"}},"outputId":"3b233aae-ddde-49a5-eeee-94e2b0d1976c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["           sum_sq    df          F        PR(>F)\n","C(region)  1000.0   2.0  67.873303  2.870664e-07\n","Residual     88.4  12.0        NaN           NaN\n","There is a statistically significant difference in average heights between the regions.\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-2-169dd8d0496d>:25: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  if anova_table['PR(>F)'][0] < alpha:\n"]}]}]}